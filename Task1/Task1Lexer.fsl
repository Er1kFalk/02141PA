// The generated lexer module will start with this code
{
module Task1Lexer
open FSharp.Text.Lexing
open System
// open the module that defines the tokens
open Task1Parser
// Set the language to English such that 4.0 is parsed as 4 and not 40.
System.Globalization.CultureInfo.CurrentCulture <- new System.Globalization.CultureInfo("en-US")
}

// We define macros for some regular expressions we will use later
let digit       = ['0'-'9']
let num         = digit+
let var			= ['a'-'z''A'-'Z']['a'-'z''A'-'Z''0'-'9''_']*
let whitespace  = [' ' '\t' '\t' '\n']
let newline     = "\n\r" | '\n' | '\r'

// We define now the rules for recognising and building tokens
// for each of the tokens of our language we need a rule
// NOTE: rules are applied in order top-down.
//       This is important when tokens overlap (not in this example)
rule tokenize = parse
// deal with tokens that need to be ignored (skip them)
| whitespace    { tokenize lexbuf }
| newline       { lexbuf.EndPos <- lexbuf.EndPos.NextLine; tokenize lexbuf; }
// deal with tokens that need to be built
| num           { NUM(Int32.Parse(LexBuffer<_>.LexemeString lexbuf)) }
| var			{ VAR(LexBuffer<_>.LexemeString lexbuf) }
| '*'           { TIMES }
| '/'           { DIV }
| '+'           { PLUS }
| '-'           { UMINUS}
| '-'           { MINUS }
| '^'           { POW }
| '('           { LPAR }
| ')'           { RPAR }
| ":="			{ ASSIGN }
| "skip"		{ SKIP }
| ';'           { COMMANDLIST}
|  "if"			{IF}
| "fi"			{FI}
| "do"			{DO}
| "od"			{OD}
| "->"			{BFUNCTION}
| "[]"			{TWOGUARD}
| '['			{LSQPAR}
| ']'			{RSQPAR}
| "&&"          {DAND}
| '&'			{AND}
| "||"			{DOR}
| '|'			{OR}
| "!="			{NOTEQUAL}
| '!'			{NOT}
| '='			{EQUAL}
| ">="			{GEQ}
| '>'           {GT}
| "<="          {LEQ}
| '<'           {LT}
| "true"		{TRUE}
| "false"		{FALSE}
| eof           { EOF }
